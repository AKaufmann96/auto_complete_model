Я работаю над задачей по созданию нейросети для автодополнения текстов.
Эта нейросеть должна на основе начала фразы предсказывает её продолжение.

Автодополнение текста можно реализовать как на уровне символов (дополнить слово, которое печатает пользователь), так и на уровне слов (закончить текст несколькими словами или предложениями).

---

У меня есть формальное описание задачи:

1. Взять датасет от разработчиков, очистить его, подготовить для обучения модели.
2. Реализовать и обучить модель на основе рекуррентных нейронных сетей.
3. Замерить качество разработанной и обученной модели.
4. Взять более «тяжёлую» предобученную модель из Transformers и замерить её качество.
5. Проанализировать результаты и дать рекомендации разработчикам: стоит ли использовать лёгкую модель или лучше постараться поработать с ограничениями по памяти и использовать большую предобученную.

---

У меня есть структура проекта:

text-autocomplete/
├── data/                            # Датасеты
│   ├── raw_dataset.csv              # "сырой" скачанный датасет
│   └── dataset_processed.csv        # "очищенный" датасет
│   ├── train.csv                    # тренировочная выборка
│   ├── val.csv                      # валидационная выборка
│   └── test.csv                     # тестовая выборка
│
├── src/                             # Весь код проекта
│   ├── data_utils.py                # Обработка датасета
|   ├── next_token_dataset.py        # код с torch Dataset'ом 
│   ├── lstm_model.py                # код lstm модели
|   ├── eval_lstm.py                 # замер метрик lstm модели
|   ├── lstm_train.py                # код обучения модели
|   ├── eval_transformer_pipeline.py # код с запуском и замером качества трансформера
│
├── configs/                         # yaml-конфиги с настройками проекта
│
├── models/                          # веса обученных моделей
|
├── solution.ipynb                   # ноутбук с решением
└── requirements.txt                 # зависимости проекта

---

Уменя есть поэтапное выполнение проекта:

Этап 0. Подготовка окружения
Подготовьте окружение. Используйте файл requirements_sprint_2_project.txt.
Создайте git-репозиторий. Подумайте над структурой проекта. Можете заранее создать некоторые файлы, в которых будете писать код.

Этап 1. Сбор и подготовка данных
Скачайте датасет, положите его в папку data.
«Почистите» тексты в датасете, а затем токенизируйте их. Для удобства можете сохранить почищенный и токенизированный датасет.
Разбейте датасет на трейн, валидацию и тест.
Создайте torch.Dataset и torch.DataLoader для обучения модели.

Этап 2. Реализация рекуррентной сети
Напишите код модели на основе LSTM. В методе forward модель должна принимать на вход последовательность токенов и предсказывать следующий токен.
Дополнительно для модели реализуйте метод генерации нескольких токенов.

Этап 3. Тренировка модели
Напишите код замера и вывода метрики ROUGE. В коде модель должна проходиться по DataLoader'у и генерировать автодополнения, которые затем будут сравниваться с таргетом. 
Для простоты реализуйте сценарий, где в качестве входа модель получает 3/4 исходного текста и старается дополнить оставшиеся 1/4.
Напишите код тренировки модели, во время тренировки выводите значения функции потерь и метрики ROUGE.
Обучите модель, подобрав оптимальные параметры. Во время тренировки или после неё выведите некоторые примеры автодополнений, которые выучила модель. Ориентировочное время тренировки: несколько минут на одну эпоху при размере батча около 256 и размерности скрытого слоя LSTM около 128. Так как тексты ограничены по длине, а памяти на видеокарте хватает с запасом, то можно ставить размер батча и размерности модели и побольше.

Этап 4. Использование предобученного трансформера
Воспользуйтесь моделью трансформера distilgpt2 из Transformers и дополните тексты из датасета.
Напишите код замера и вывода метрики ROUGE, но уже с использованием трансформера. Здесь, как и с LSTM, предсказывайте последнюю четверть текста.
Подберите параметры генерации, замерьте качество модели на валидационной выборке, выведите примеры предсказаний.

Этап 5. Формулирование выводов
Сравните примеры предсказаний двух моделей, а также получившиеся метрики.
Сделайте выводы о том, какую модель лучше использовать и почему.

---

Твоя задача - поэтапно помочь мне выполнить необходимые этапы реализации проекта, ориентируюясь на предоставленную выше информацию. Не пиши весь необходимый код сразу. Снача пиши какой этап мы выполняем, затем говори какой файл из структуры мы будет писать, а затем выдавай необходимый код.

Я работаю пока локально и загружу в git уже готовый репозиторий. Я уже локально воссаздал структуру проекта из инструкции, датасет у меня хранится в файле raw_dataset.csv в папке data.