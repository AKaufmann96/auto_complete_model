src

Файл: src/data_utils.py
На данном этапе мы реализуем функции для очистки текста, токенизации и разбиения датасета на обучающую, валидационную и тестовую выборки. Данные будут сохранены в соответствующие CSV-файлы.

-

Файл: src/next_token_dataset.py
На этом шаге мы реализуем кастомный torch.utils.data.Dataset, который будет:

Загружать очищенные тексты из разбитых CSV-файлов (train.csv, val.csv, test.csv);
Преобразовывать тексты в последовательности токенов;
Строить словарь (токен → индекс);
Подготавливать пары: входная последовательность токенов → следующий токен (цель).

-

src/lstm_model.py
На данном этапе мы реализуем модель на основе LSTM для предсказания следующего токена. Модель будет:

Принимать на вход последовательность индексов токенов;
Преобразовывать их в эмбеддинги;
Пропускать через один или несколько LSTM-слоёв;
Использовать последнее скрытое состояние для предсказания следующего токена через полносвязный слой.

-

Файл: src/lstm_train.py
На данном этапе мы реализуем код для:

загрузки данных через get_dataloader;
инициализации модели LSTM;
настройки оптимизатора и функции потерь;
обучения модели с выводом лосса и метрик;
сохранения обученной модели.
Также добавим вывод примеров автодополнения на валидации после каждой эпохи.

-

Файл: src/eval_lstm.py
На данном этапе мы реализуем:

функцию evaluate_model — вычисляет loss и accuracy на валидационной/тестовой выборке;
функцию compute_rouge_scores — вычисляет метрику ROUGE (ROUGE-1, ROUGE-2, ROUGE-L) между сгенерированными и целевыми продолжениями;
функцию generate_completion — генерирует продолжение текста на основе первых 3/4 исходного текста, как указано в задании;
функцию generate_examples — выводит примеры автодополнения.

-

Файл: src/eval_transformer_pipeline.py
На данном этапе мы:

Загрузим предобученную модель DistilGPT-2 из библиотеки transformers;
Настроим токенизатор;
Реализуем генерацию продолжения текста по первым 3/4 исходного текста;
Оценим качество модели с помощью ROUGE на валидационной выборке;
Сравним результаты с LSTM-моделью.
Мы будем использовать distilgpt2 — более лёгкую версию GPT-2, подходящую для сравнения с LSTM по качеству при меньших вычислительных затратах.